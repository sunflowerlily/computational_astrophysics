{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1487baff-0d76-4979-8f07-73f7bce03859",
   "metadata": {},
   "source": [
    "# Application: Galaxy Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7684a-6247-4fbc-8be1-d520233c0b0b",
   "metadata": {},
   "source": [
    "Galaxies come in [many different shapes](https://en.wikipedia.org/wiki/Galaxy_morphological_classification).  Large surveys can images 100,000s or millions of galaxies, and we want to be able to classify them into types.  We can use a neural network for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bce080-6fda-41b4-b7a0-e9b823677c32",
   "metadata": {},
   "source": [
    "We will use the [Galaxy10 DECaLS dataset](https://astronn.readthedocs.io/en/latest/galaxy10.html) [(Leung & Bovy 2024)](https://zenodo.org/records/10845026),\n",
    "which contains 17736 images of galaxies, each 256x256 pixels and 3 colors (g, r, and z bands), classified into 10 different categories.  It also provides RA and dec, redshift, and pixel scale---we'll ignore those.\n",
    "\n",
    "You can get the dataset from Zenodo: [Galaxy10_DECals.h5](https://zenodo.org/records/10845026/files/Galaxy10_DECals.h5)\n",
    "\n",
    "This is an [HDF5](https://www.hdfgroup.org/solutions/hdf5/) file---this is a self-describing binary format that is popular in scientific computing.  We can read it in python using the\n",
    "[h5py library](https://www.h5py.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe02d14-a08e-4bf7-8888-93d46e806ac1",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "This is a big dataset (~2.5 GB), so we need to be careful with memory.  As we'll see below,\n",
    "we'll keep the data in the original `uint8` form for as long as possible, converting it\n",
    "to a float a galaxy at a time, as needed.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac2550-f8e0-4c00-8490-f44329288e99",
   "metadata": {},
   "source": [
    "```{note}\n",
    "There is no separate training and test set, so we need to partition the 17736 images ourselves.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e7b51fe-b413-4e43-8d30-3e39c5699828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1ced9-e490-4776-8e88-4b9b24dbc196",
   "metadata": {},
   "source": [
    "If we look at the dataset, we can see the fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a110f0a5-6247-4001-bb67-829df01efb25",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'Galaxy10_DECals.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGalaxy10_DECals.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m ds\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/h5py/_hl/files.py:564\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    555\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    556\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    557\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    558\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    559\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    560\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    561\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    562\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    563\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 564\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/h5py/_hl/files.py:238\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    237\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 238\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    240\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'Galaxy10_DECals.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "ds = h5py.File(\"Galaxy10_DECals.h5\")\n",
    "ds.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83205aa-8f46-45fb-923f-b5f54a74ff99",
   "metadata": {},
   "source": [
    "let's look at the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8a6b45-cbc5-46dc-9ae4-ef408fbdcc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = np.array(ds[\"ans\"])\n",
    "ans.shape, ans.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612f7c8-3a22-429b-b6d9-d1d137f74a7d",
   "metadata": {},
   "source": [
    "We see that there are 17736 records, stored as a NumPy array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246de1d-4883-42a0-aaf4-41c1a819abc1",
   "metadata": {},
   "source": [
    "```{important}\n",
    "The galaxies are stored in the dataset sorted by type.  So all of the \"disturbed galaxies\" (type `0`) come first, then the \"merging galaxies\", ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cde0ba-8556-4c5c-a286-d05462b4c878",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f209b8b7-6dce-431a-9965-1b9fd225abd5",
   "metadata": {},
   "source": [
    "Let's define descriptive names for the galaxy types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8adee18-1502-48b0-928a-c51a6038ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy_types = {0: \"disturbed galaxies\",\n",
    "                1: \"merging galaxies\",\n",
    "                2: \"round smooth galaxies\",\n",
    "                3: \"in-between round smooth galaxies\",\n",
    "                4: \"cigar shaped smooth galaxies\",\n",
    "                5: \"barred spiral galaxies\",\n",
    "                6: \"unbarred tight spiral galaxies\",\n",
    "                7: \"unbarred loose spiral galaxies\",\n",
    "                8: \"edge-on galaxies without bulge\",\n",
    "                9: \"edge-on galaxies with bulge\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632d4e97-51a1-4e1d-961e-dc0b28f98eef",
   "metadata": {},
   "source": [
    "We'll also create a simple class (`Galaxy`) to manage the data.\n",
    "\n",
    "Here we pass in the image data for a single galaxy as a NumPy `uint8` array---the same datatype used in the file.  This class will convert it to a single precision floating-point array, scaled to fall between 0 and 1.  We also convert the answer to a categorical datatype (and array of length 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152544c-653f-4422-9b78-563d5b6c4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Galaxy:\n",
    "    def __init__(self, data, answer, *, index=-1):\n",
    "        self.data = np.array(data, dtype=np.float32) / 255.0 * 0.99 + 0.01\n",
    "        self.answer = answer\n",
    "\n",
    "        self.out = np.zeros(10, dtype=np.float32) + 0.01\n",
    "        self.out[self.answer] = 0.99\n",
    "\n",
    "        self.index = index\n",
    "\n",
    "    def plot(self, ax=None):\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "        ax.imshow(self.data, interpolation=\"nearest\")\n",
    "        ax.text(0.025, 0.95, f\"answer: {self.answer}\",\n",
    "                color=\"white\", transform=ax.transAxes)\n",
    "\n",
    "    def validate(self, prediction):\n",
    "        \"\"\"check if a categorical prediction matches the answer\"\"\"\n",
    "        return np.argmax(prediction) == self.answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0db920-6e72-4a2f-8402-03eb4cd272a1",
   "metadata": {},
   "source": [
    "Now let's find the first example of each type and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4cd0f-ba6f-4b90-99c5-f3ef292bdce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = ds[\"images\"]\n",
    "images.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5be5a2-de19-446e-ba76-2d276677b1c9",
   "metadata": {},
   "source": [
    "```{note}\n",
    "This doesn't read in the data.  It just provides a `Dataset` type that can be used\n",
    "to access the data, so we can get a single galaxy at a time.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3acdf4-6d12-4a26-a2b7-45bc66ed9c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "fig = plt.figure()\n",
    "grid = ImageGrid(fig, 111,\n",
    "                 nrows_ncols=(4, 3),\n",
    "                 axes_pad=(0.25, 0.5))\n",
    "\n",
    "for i in range(10):\n",
    "    idx = np.where(ans == i)[0][0]\n",
    "    g = Galaxy(images[idx, :, :, :], ans[idx])\n",
    "    g.plot(ax=grid[i])\n",
    "    grid[i].set_title(f\"{idx}: {galaxy_types[i]}\", fontsize=\"small\")\n",
    "\n",
    "grid[10].set_axis_off()\n",
    "grid[11].set_axis_off()\n",
    "fig.set_size_inches(10, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa549b0-ae31-4c79-9139-c49a7f4587dd",
   "metadata": {},
   "source": [
    "## A manager class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bbd112-ed92-45ee-a0f1-50d869304aed",
   "metadata": {},
   "source": [
    "We'll create a class to manage access to the data.  This will do the following:\n",
    "\n",
    "* open the file and store the handles to access the data\n",
    "* partition the data into test and training sets\n",
    "* provide a means to shuffle the data\n",
    "* provide methods to get the next dataset (either training or test)\n",
    "* allow us to coarsen the images to a reduced resolution to make the training easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8e361-a5c8-41bc-a3bb-90ad01196378",
   "metadata": {},
   "source": [
    "```{important}\n",
    "Since each class of galaxy is stored together in the file, we will want to\n",
    "randomize the order before splitting into training and testing sets.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a2ae0e-82ee-4f85-8cf4-0ad3a3c398e0",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "We'll provide 2 different ways of managing access to the data.  If `read_on_demand=True` is set,\n",
    "then we only read the image data one at a time as needed to keep the memory requirements low.  This\n",
    "uses a lot less memory, but the bulk of the training time will be spent on reading, and will take\n",
    "more than an order of magnitude longer.\n",
    "\n",
    "The default is to read all of the data once, which will take about 3.5 GB of memory.  It is kept\n",
    "as a `uint8` until needed.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8512ea-bbe6-4d53-8ee2-f372320f41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, partition=0.8,\n",
    "                 datafile=\"Galaxy10_DECals.h5\",\n",
    "                 coarsen=1,\n",
    "                 read_on_demand=False):\n",
    "        \"\"\"manage access to the data\n",
    "\n",
    "        partition: fraction that should be training\n",
    "        datafile: name of the hdf5 file with the data\n",
    "        coarsen: reduce the number of pixels by this factor\n",
    "        read_on_demand: do a only read the data from disk as needed?\n",
    "        \"\"\"\n",
    "\n",
    "        self.ds = h5py.File(datafile)\n",
    "        self.ans = np.array(self.ds[\"ans\"])\n",
    "        if read_on_demand:\n",
    "            self.images = self.ds[\"images\"]\n",
    "        else:\n",
    "            self.images = np.array(self.ds[\"images\"])\n",
    "\n",
    "        self.coarsen = coarsen\n",
    "\n",
    "        N = len(self.ans)\n",
    "\n",
    "        # create a set of indices for the galaxies and randomize\n",
    "        self.indices = np.arange(N, dtype=np.uint32)\n",
    "        self.rng = np.random.default_rng()\n",
    "        self.rng.shuffle(self.indices)\n",
    "\n",
    "        # partition into training and test sets\n",
    "        # these indices will always refer to the index in the original\n",
    "        # unsplit dataset\n",
    "        n_cut = int(partition * N)\n",
    "        self.training_indices = self.indices[0:n_cut]\n",
    "        self.test_indices = self.indices[n_cut:N]\n",
    "\n",
    "        self.n_training = len(self.training_indices)\n",
    "        self.n_test = len(self.test_indices)\n",
    "        \n",
    "        # store the current index into the *_indices array we are\n",
    "        # accessing\n",
    "        self.curr_idx_train = -1\n",
    "        self.curr_idx_test = -1\n",
    "\n",
    "    def _get_galaxy(self, index):\n",
    "        \"\"\"return a numpy array containing a single galaxy image, coarsened\n",
    "        if necessary by averaging\"\"\"\n",
    "        _tmp = self.images[index, :, :, :]\n",
    "        if self.coarsen > 1:\n",
    "            _tmp = np.mean(_tmp.reshape(_tmp.shape[0]//self.coarsen, self.coarsen,\n",
    "                                        _tmp.shape[1]//self.coarsen, self.coarsen,\n",
    "                                        _tmp.shape[2]), axis=(1, 3))\n",
    "        return _tmp\n",
    "\n",
    "    def get_next_training_image(self):\n",
    "        self.curr_idx_train += 1\n",
    "        if self.curr_idx_train < len(self.training_indices):\n",
    "            idx = self.training_indices[self.curr_idx_train]\n",
    "            return Galaxy(self._get_galaxy(idx), self.ans[idx], index=idx)\n",
    "        return None\n",
    "\n",
    "    def reset_training(self):\n",
    "        \"\"\"prepare for the next epoch: shuffle the training data and\n",
    "        reset the index to point to the start\"\"\"\n",
    "        self.curr_idx_train = -1\n",
    "        self.rng.shuffle(self.training_indices)\n",
    "\n",
    "    def reset_testing(self):\n",
    "        \"\"\"reset the pointer for the test data\"\"\"\n",
    "        self.curr_idx_test = -1\n",
    "\n",
    "    def get_next_test_image(self):\n",
    "        self.curr_idx_test += 1\n",
    "        if self.curr_idx_test < len(self.test_indices):\n",
    "            idx = self.test_indices[self.curr_idx_test]\n",
    "            return Galaxy(self._get_galaxy(idx), self.ans[idx], index=idx)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3085a0eb-9384-4e8e-a982-709a010dbb0e",
   "metadata": {},
   "source": [
    "````{tip}\n",
    "The `get_next_training_image()` and `get_next_test_image()` will return `None` when there are no more galaxies.\n",
    "This allows us to loop over the data set as:\n",
    "\n",
    "```python\n",
    "d = DataManager()\n",
    "while g := d.get_next_training_image():\n",
    "    # do stuff with g\n",
    "```\n",
    "\n",
    "where we use the [python walrus operator](https://peps.python.org/pep-0572/), `:=` to assign to `g` within the\n",
    "loop conditional.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592203a0-8335-48ad-a174-0eb92a405cca",
   "metadata": {},
   "source": [
    "We can now work with the data as follows.  Here we create a `DataManager` that will coarsen the images by a factor of 4 (so they will be 64x64 pixels with 3 colors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e7c95-aeb9-4186-9d82-a11497f4a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DataManager(coarsen=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdb9c2-c3f5-4451-838c-3ab81766eb26",
   "metadata": {},
   "source": [
    "we can see how many images there are in the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381963b-c8c5-41a9-94a1-6f9930557087",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.n_training, d.n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14232322-934e-4278-85a2-ee40353afe8f",
   "metadata": {},
   "source": [
    "We can then get a training galaxy and look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa08663e-7019-435a-a29e-2ccdced8d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = d.get_next_test_image()\n",
    "g.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef947c-a633-4185-803b-1aeb3ce3316d",
   "metadata": {},
   "source": [
    "We'll need a 1-d representation of the data, which we can get using `np.ravel()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cdd236-2124-4843-a668-a655e47593f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ravel(g.data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e984395-c6b5-4707-8bec-3fc54edc278f",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86160e-e3fe-4468-929e-54f89b83944b",
   "metadata": {},
   "source": [
    "Training with this data will be very slow.  We can speed it up more by using batching and aggregating the linear algebra.  Here's how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b70dc0-eec1-43bf-ad0d-b5fb35c213d5",
   "metadata": {},
   "source": [
    "### Single input recap\n",
    "\n",
    "Our basic network does:\n",
    "\n",
    "$$\\tilde{\\bf z}^k = g({\\bf B} {\\bf x}^k)$$\n",
    "\n",
    "$${\\bf z}^k = g({\\bf A} \\tilde{\\bf z}^k)$$\n",
    "\n",
    "where the sizes of the matrices and vectors are:\n",
    "\n",
    "* ${\\bf x}^k$ : $N_\\mathrm{in} \\times 1$\n",
    "* ${\\bf B}$ : $N_\\mathrm{hidden} \\times N_\\mathrm{in}$\n",
    "* $\\tilde{\\bf z}^k$ : $N_\\mathrm{hidden}\\times 1$\n",
    "* ${\\bf A}$ : $N_\\mathrm{out} \\times N_\\mathrm{hidden}$\n",
    "* ${\\bf z}^k$ : $N_\\mathrm{out} \\times 1$\n",
    "\n",
    "we also have the known output, ${\\bf y}^k$ corresponding to input ${\\bf x}^k$\n",
    "\n",
    "* ${\\bf y}^k$ : $N_\\mathrm{out} \\times 1$\n",
    "\n",
    "we then compute the errors:\n",
    "\n",
    "* ${\\bf e}^k = {\\bf z}^k - {\\bf y}^k$ (the error on the output layer) : $N_\\mathrm{out} \\times 1$\n",
    "* $\\tilde{\\bf e}^k = {\\bf A}^\\intercal \\cdot [{\\bf e}^k \\circ {\\bf z} \\circ (1 - {\\bf z})]$ (the error backpropagated to the hidden layer) : $N_\\mathrm{hidden} \\times 1$\n",
    "\n",
    "and finally the corrections due to this single piece of training data, $({\\bf x}^k, {\\bf y}^k)$:\n",
    "\n",
    "* $\\Delta {\\bf A} = -2\\eta \\,{\\bf e}^k \\circ {\\bf z}^k \\circ (1 - {\\bf z}^k) \\cdot (\\tilde{\\bf z}^k)^\\intercal$ : $N_\\mathrm{out} \\times N_\\mathrm{hidden}$\n",
    "* $\\Delta {\\bf B} = -2\\eta \\,\\tilde{\\bf e}^k \\circ \\tilde{\\bf z}^k \\circ (1 - \\tilde{\\bf z}^k) \\cdot ({\\bf x}^k)^\\intercal$ : $N_\\mathrm{hidden} \\times N_\\mathrm{in}$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a7874-954c-44a0-bda2-ca248cdb4609",
   "metadata": {},
   "source": [
    "### A batching approach\n",
    "\n",
    "We now want to batch the inputs, by extending ${\\bf x}$ to be of size $N_\\mathrm{in} \\times S$, where $S$ is the batch size.  This means that each column is a unique\n",
    "input vector ${\\bf x}^k$, and $S$ of them are sandwiched together:\n",
    "\n",
    "$${\\bf x}_b = \\left ( \\begin{array}{ccccc}\n",
    "                      | & | & | &  & | \\\\\n",
    "                      {\\bf x}^0 & {\\bf x}^1 & {\\bf x}^2 & ... & {\\bf x}^{S-1} \\\\\n",
    "                      | & | & | &  & | \n",
    "                      \\end{array} \\right )$$\n",
    "\n",
    "Similarly, we create a batched ${\\bf y}_b$ that contains the ${\\bf y}^k$ corresponding\n",
    "to the ${\\bf x}^k$ in ${\\bf x}_b$.\n",
    "\n",
    "We can propagate this through the network, getting\n",
    "\n",
    "$${\\bf z}_b = g({\\bf A} g({\\bf B}{\\bf x}_b))$$\n",
    "\n",
    "where ${\\bf z}_b$ is now of size $N_\\mathrm{out} \\times S$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ba47d-31fc-4caa-aebe-032d793d56e6",
   "metadata": {},
   "source": [
    "Now, we compute the errors from the batched inputs\n",
    "\n",
    "* ${\\bf e}_b = {\\bf z}_b - {\\bf y}_b$ : $N_\\mathrm{out} \\times S$\n",
    "* $\\tilde{\\bf e}_b =\\underbrace{{\\bf A}^\\intercal}_{N_\\mathrm{hidden} \\times N_\\mathrm{out}} \\cdot \\underbrace{[{\\bf e}_b \\circ {\\bf z}_b \\circ (1 - {\\bf z})]}_{N_\\mathrm{out} \\times S}$ : $N_\\mathrm{hidden} \\times S$\n",
    "\n",
    "and the accumulated corrections:\n",
    "\n",
    "* $\\Delta {\\bf A} = -\\frac{2\\eta}{S} \\,\\underbrace{{\\bf e}_b \\circ {\\bf z}_b \\circ (1 - {\\bf z}_b)}_{N_\\mathrm{out}\\times S} \\cdot \\underbrace{(\\tilde{\\bf z}_b)^\\intercal}_{S\\times N_\\mathrm{hidden}}$\n",
    "* $\\Delta {\\bf B} = -\\frac{2\\eta}{S} \\,\\underbrace{\\tilde{\\bf e}_b \\circ \\tilde{\\bf z}_b \\circ (1 - \\tilde{\\bf z}_b)}_{N_\\mathrm{hidden} \\times S} \\cdot \\underbrace{({\\bf x}_b)^\\intercal}_{S \\times N_\\mathrm{in}}$\n",
    "\n",
    "In these accumulated corrections, the $S$ dimensions contract.  In essence, this means\n",
    "that each element in $\\Delta {\\bf A}$ and $\\Delta {\\bf B}$ is the sum of the corrections for each of the $S$ training data pairs in the batch.  For this reason, we normalize by $S$ to create the average of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a8dd8-b866-4ba6-8827-515047706571",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Batching also stabilizes the gradient descent, making it easier to find\n",
    "the minimum and allowing us to use a larger learning rate.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6346e2b-e24c-43d8-9b76-88027059d93f",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd8c20-3c81-4c07-ab69-ad1a88e63508",
   "metadata": {},
   "source": [
    "The other feature we need for this application is [momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) in the gradient descent weight updates.\n",
    "\n",
    "A popular form of momentum (see, e.g., [Momentum: A simple, yet efficient optimizing technique](https://medium.com/analytics-vidhya/momentum-a-simple-yet-efficient-optimizing-technique-ef76834e4423)) builds off of the idea of the [exponential moving average](https://en.wikipedia.org/wiki/Exponential_smoothing).\n",
    "\n",
    "\n",
    "For our gradient descent update, we usually do:\n",
    "\n",
    "$${\\bf A} = {\\bf A} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial {\\bf A}}$$\n",
    "\n",
    "where $\\mathcal{L}$ is our loss function and $\\eta$ is the learning rate.\n",
    "\n",
    "The basic idea of momentum begins with defining a \"velocity\", ${\\bf v}^{(0)} = 0$ (no momentum has been built up yet).\n",
    "Then each iteration of training we do the following:\n",
    "\n",
    "* construct the gradient from the current set of training, $\\partial \\mathcal{L}/\\partial {\\bf A}$\n",
    "\n",
    "* blend this with the previous momentum using an exponential moving average:\n",
    "\n",
    "  $${\\bf v}^{(i)} = \\beta {\\bf v}^{(i-1)} + (1 - \\beta) \\frac{\\partial \\mathcal{L}}{\\partial {\\bf A}}$$\n",
    "\n",
    "  where $\\beta \\in [0, 1]$ is the smoothing parameter.  It seems like $\\beta = 0.9$ is used often.\n",
    "\n",
    "  \n",
    "  Since every gradient is always multiplied by $(1-\\beta)$,\n",
    "  and each previous gradient picks up a factor of $\\beta$\n",
    "  each iteration, this construction weights the most recent gradients most.\n",
    "   \n",
    "* update the weights:\n",
    "\n",
    "  $${\\bf A} = {\\bf A} - \\eta {\\bf v}^{(i)}$$\n",
    "\n",
    "We would do the same with the other weights, ${\\bf B}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4a4a4-1caf-435c-bbfe-dc297eff8858",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Momentum greatly reduces the swings in the \"fraction correct\" metric from one epoch to the next.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a61fc4-2dc3-4a9e-ba36-35abc4e6ca3b",
   "metadata": {},
   "source": [
    "## Implementing our neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d94b7d-7675-45da-9378-4be88159a810",
   "metadata": {},
   "source": [
    "We'll write our network to take a `DataManager`---it can get everything that it needs from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd447c-13fd-4750-9fc9-347b40fe3439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53605e83-ccd3-406c-b7f6-597f381943f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"A neural network class with a single hidden layer.\"\"\"\n",
    "\n",
    "    def __init__(self, data_manager, *, hidden_layer_size=20):\n",
    "\n",
    "        self.data_manager = data_manager\n",
    "\n",
    "        # let's get the first image from the training set and\n",
    "        # use that to set the sizes\n",
    "        g = self.data_manager.get_next_training_image()\n",
    "\n",
    "        # the number of nodes/neurons on the output layer\n",
    "        self.N_out = g.out.size\n",
    "\n",
    "        # the number of nodes/neurons on the input layer\n",
    "        self.N_in = np.ravel(g.data).size\n",
    "\n",
    "        # the number of nodes/neurons on the hidden layer\n",
    "        self.N_hidden = hidden_layer_size\n",
    "\n",
    "        # we will initialize the weights with Gaussian normal random\n",
    "        # numbers centered on 0 with a width of 1/sqrt(n), where n is\n",
    "        # the length of the input state\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "        # A is the set of weights between the hidden layer and output layer\n",
    "        self.A = np.zeros((self.N_out, self.N_hidden), dtype=np.float32)\n",
    "        self.A[:, :] = rng.normal(0.0, 1.0/np.sqrt(self.N_hidden), self.A.shape)\n",
    "\n",
    "        # B is the set of weights between the input layer and hidden layer\n",
    "        self.B = np.zeros((self.N_hidden, self.N_in), dtype=np.float32)\n",
    "        self.B[:, :] = rng.normal(0.0, 1.0/np.sqrt(self.N_in), self.B.shape)\n",
    "        \n",
    "        # reset the training\n",
    "        self.data_manager.reset_training()\n",
    "\n",
    "        self.n_trained = 0\n",
    "        self.training_time = 0\n",
    "\n",
    "    def sigmoid(self, xi):\n",
    "        \"\"\"our sigmoid function that operates on the hidden layer\"\"\"\n",
    "        return 1.0/(1.0 + np.exp(-xi))\n",
    "\n",
    "    def _batch_update(self, x_batch, y_batch):\n",
    "\n",
    "        # batch size\n",
    "        S = len(x_batch)\n",
    "\n",
    "        x = np.array(x_batch).T\n",
    "        y = np.array(y_batch).T\n",
    "\n",
    "        # propagate the input through the network\n",
    "        z_tilde = self.sigmoid(self.B @ x)\n",
    "        z = self.sigmoid(self.A @ z_tilde)\n",
    "\n",
    "        # compute the errors (backpropagate to the hidden layer)\n",
    "        e = z - y\n",
    "        e_tilde = self.A.T @ (e * z * (1 - z))\n",
    "\n",
    "        # corrections\n",
    "        grad_A = (2/S) * e * z * (1 - z) @ z_tilde.T\n",
    "        grad_B = (2/S) * e_tilde * z_tilde * (1 - z_tilde) @ x.T\n",
    "\n",
    "        self.n_trained += S\n",
    "        \n",
    "        return grad_A, grad_B\n",
    "\n",
    "    def assess(self):\n",
    "        \"\"\"Run through the test data and return the fraction correct\n",
    "        with the currently trained network\"\"\"\n",
    "             \n",
    "        self.data_manager.reset_testing()\n",
    "        n_correct = 0\n",
    "        while gt := self.data_manager.get_next_test_image():\n",
    "            ans = self.predict(gt)\n",
    "            if gt.validate(ans):\n",
    "                n_correct += 1\n",
    "\n",
    "        return n_correct / self.data_manager.n_test\n",
    "\n",
    "    def train(self, *, n_epochs=1,\n",
    "              learning_rate=0.2, beta_momentum=0.9,\n",
    "              batch_size=64):\n",
    "        \"\"\"Train the neural network by doing gradient descent with back\n",
    "        propagation to set the matrix elements in B (the weights\n",
    "        between the input and hidden layer) and A (the weights between\n",
    "        the hidden layer and output layer)\n",
    "        \"\"\"\n",
    "\n",
    "        v_A = np.zeros_like(self.A)\n",
    "        v_B = np.zeros_like(self.B)\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "\n",
    "            start = time.perf_counter()\n",
    "\n",
    "            self.data_manager.reset_training()\n",
    "\n",
    "            # storage for our batches\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "\n",
    "            while g := self.data_manager.get_next_training_image():\n",
    "\n",
    "                # make a 1-d representation of the input, called x, and call\n",
    "                # the output y\n",
    "                x_batch.append(np.ravel(g.data))\n",
    "                y_batch.append(g.out)\n",
    "\n",
    "                if len(x_batch) == batch_size:\n",
    "                    # batch is full -- do the training\n",
    "                    grad_A, grad_B = self._batch_update(x_batch, y_batch)\n",
    "\n",
    "                    v_A[...] = beta_momentum * v_A + (1.0 - beta_momentum) * grad_A\n",
    "                    v_B[...] = beta_momentum * v_B + (1.0 - beta_momentum) * grad_B\n",
    "\n",
    "                    self.A[...] += -learning_rate * v_A\n",
    "                    self.B[...] += -learning_rate * v_B\n",
    "\n",
    "                    x_batch = []\n",
    "                    y_batch = []\n",
    "\n",
    "            # we may have run out of data without filling up the\n",
    "            # last batch, so take care of that now\n",
    "            if x_batch:\n",
    "                grad_A, grad_B = self._batch_update(x_batch, y_batch)\n",
    "\n",
    "                v_A[...] = beta_momentum * v_A + (1.0 - beta_momentum) * grad_A\n",
    "                v_B[...] = beta_momentum * v_B + (1.0 - beta_momentum) * grad_B\n",
    "\n",
    "                self.A[...] += -learning_rate * v_A\n",
    "                self.B[...] += -learning_rate * v_B\n",
    "\n",
    "            epoch_time = time.perf_counter() - start\n",
    "            self.training_time += epoch_time\n",
    "\n",
    "            frac_correct = self.assess()\n",
    "\n",
    "            print(f\"epoch {i+1:3} | \" +\n",
    "                  f\"test set correct: {frac_correct:5.3f}; \" +\n",
    "                  f\"training time: {epoch_time:7.3f} s\")\n",
    "\n",
    "    def predict(self, model):\n",
    "        \"\"\" predict the outcome using our trained matrix A \"\"\"\n",
    "        x_in = np.ravel(model.data)[:, np.newaxis]\n",
    "        y = self.sigmoid(self.A @ (self.sigmoid(self.B @ x_in)))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb961e-3825-4179-828e-29e9e976fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(d, hidden_layer_size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee05a6a-4764-4366-80b0-987e3b4cb00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.train(n_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f604c36c-8024-4aa5-84fe-355cc9802d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.n_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d4b392-33e5-4f82-a48f-53bc2410fe2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
